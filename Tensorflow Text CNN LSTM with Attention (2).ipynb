{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['ubuntu']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.contrib import rnn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import os\n",
    "print(os.listdir(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('~/test_data.json', lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 41 category\n",
      "category\n",
      "ARTS               1509\n",
      "ARTS & CULTURE     1339\n",
      "BLACK VOICES       4528\n",
      "BUSINESS           5937\n",
      "COLLEGE            1144\n",
      "COMEDY             5175\n",
      "CRIME              3405\n",
      "CULTURE & ARTS     1030\n",
      "DIVORCE            3426\n",
      "EDUCATION          1004\n",
      "ENTERTAINMENT     16058\n",
      "ENVIRONMENT        1323\n",
      "FIFTY              1401\n",
      "FOOD & DRINK       6226\n",
      "GOOD NEWS          1398\n",
      "GREEN              2622\n",
      "HEALTHY LIVING     6694\n",
      "HOME & LIVING      4195\n",
      "IMPACT             3459\n",
      "LATINO VOICES      1129\n",
      "MEDIA              2815\n",
      "MONEY              1707\n",
      "PARENTING          8677\n",
      "PARENTS            3955\n",
      "POLITICS          32739\n",
      "QUEER VOICES       6314\n",
      "RELIGION           2556\n",
      "SCIENCE            2178\n",
      "SPORTS             4884\n",
      "STYLE              2254\n",
      "STYLE & BEAUTY     9649\n",
      "TASTE              2096\n",
      "TECH               2082\n",
      "THE WORLDPOST      3664\n",
      "TRAVEL             9887\n",
      "WEDDINGS           3651\n",
      "WEIRD NEWS         2670\n",
      "WELLNESS          17827\n",
      "WOMEN              3490\n",
      "WORLD NEWS         2177\n",
      "WORLDPOST          2579\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cates = data.groupby('category')\n",
    "print(\"There are:\", cates.ngroups, \"category\")\n",
    "print(cates.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category = data.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>[74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>[42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>[5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>[2710, 13374, 3596, 64143, 2295, 13055, 5, 569...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>[41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                               words  word_length  \n",
       "0  [74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...           27  \n",
       "1  [42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...           20  \n",
       "2  [5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...           25  \n",
       "3  [2710, 13374, 3596, 64143, 2295, 13055, 5, 569...           26  \n",
       "4  [41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...           26  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Change the text to tokens.\n",
    "data['text'] = data.headline + \" \" + data.short_description\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.text)\n",
    "X = tokenizer.texts_to_sequences(data.text)\n",
    "data['words'] = X\n",
    "\n",
    "data['word_length'] = data.words.apply(lambda i: len(i))\n",
    "data = data[data.word_length >= 5]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    199914.000000\n",
       "mean         29.725032\n",
       "std          14.024717\n",
       "min           5.000000\n",
       "25%          20.000000\n",
       "50%          29.000000\n",
       "75%          36.000000\n",
       "max         248.000000\n",
       "Name: word_length, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.word_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "X = list(sequence.pad_sequences(data.words, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "categories = data.groupby('category').size().index.tolist()\n",
    "category_int = {}\n",
    "int_category = {}\n",
    "# build category to int dict and int to category dict.\n",
    "for i, k in enumerate(categories):\n",
    "    int_category.update({i:k})\n",
    "    category_int.update({k:i})\n",
    "                                    \n",
    "data['c2id'] = data['category'].apply(lambda x: category_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>word_length</th>\n",
       "      <th>c2id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>[74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>[42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>[5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>[2710, 13374, 3596, 64143, 2295, 13055, 5, 569...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>[41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                               words  word_length  c2id  \n",
       "0  [74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...           27     6  \n",
       "1  [42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...           20    10  \n",
       "2  [5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...           25    10  \n",
       "3  [2710, 13374, 3596, 64143, 2295, 13055, 5, 569...           26    10  \n",
       "4  [41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...           26    10  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_setence(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = data[\"text\"]\n",
    "\n",
    "words = list()\n",
    "for test in texts:\n",
    "    for word in word_tokenize(clean_setence(test)):\n",
    "        words.append(word)\n",
    "\n",
    "word_counter = collections.Counter(words).most_common()\n",
    "word_dictionary = dict()\n",
    "word_dictionary[\"<padding>\"] = 0\n",
    "word_dictionary[\"<unk>\"] = 1\n",
    "for word, _ in word_counter:\n",
    "    word_dictionary[word] = len(word_dictionary)\n",
    "\n",
    "with open(\"word_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word_dictionary, f)\n",
    "\n",
    "reversed_dictionary = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "print(len(reversed_dictionary))\n",
    "doc_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Split data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(lambda d: word_tokenize(clean_setence(d)), data[\"text\"]))\n",
    "x = list(map(lambda d: list(map(lambda w: word_dictionary.get(w, word_dictionary[\"<unk>\"]), d)), x))\n",
    "x = list(map(lambda d: d[:doc_max_len], x))\n",
    "x = list(map(lambda d: d + (doc_max_len - len(d)) * [word_dictionary[\"<padding>\"]], x))\n",
    "\n",
    "y = list(data[\"c2id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test set are prepared.\n"
     ]
    }
   ],
   "source": [
    "print(\"train and test set are prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalRNN(object):\n",
    "    def __init__(self, reversed_dictionary, doc_max_len, num_class):\n",
    "        self.x = tf.placeholder(tf.int32, [None, doc_max_len], name=\"x\")\n",
    "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
    "        self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        print(\"Run BidirectionalRNN...\")\n",
    "        self.vocabulary_size = len(reversed_dictionary)\n",
    "        self.embedding_size = 100\n",
    "        self.num_hidden = 100\n",
    "        self.num_layers = 2\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        print(\"Loading Glove vectors...\")\n",
    "        glove_file = \"glove.6B.%dd.txt\" % self.embedding_size\n",
    "        word2vec_file = get_tmpfile(\"word2vec_format.vec\")\n",
    "        glove2word2vec(glove_file, word2vec_file)\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
    "\n",
    "        word_vec_list = list()\n",
    "        for _, word in sorted(reversed_dictionary.items()):\n",
    "            try:\n",
    "                word_vec = word_vectors.word_vec(word)\n",
    "            except KeyError:\n",
    "                word_vec = np.zeros([self.embedding_size], dtype=np.float32)\n",
    "\n",
    "            word_vec_list.append(word_vec)\n",
    "\n",
    "\n",
    "        print(\"start embedding...\")\n",
    "        init_embeddings = tf.constant(np.array(word_vec_list), dtype=tf.float32)\n",
    "        self.embeddings1 = tf.get_variable(\"embeddings1\", initializer=init_embeddings, trainable=True)\n",
    "        self.x_emb = tf.nn.embedding_lookup(self.embeddings1, self.x)\n",
    "\n",
    "        forward_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "        bw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "        forward_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in forward_cells]\n",
    "        bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "        self.rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n",
    "            forward_cells, bw_cells, self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
    "        self.last_output = self.rnn_outputs[:, -1, :]\n",
    "        self.logits = tf.contrib.slim.fully_connected(self.last_output, num_class, activation_fn=None)\n",
    "        self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "        # make prediction\n",
    "        correct_predictions = tf.equal(self.predictions, self.y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(inputs, outputs, batch_size, epochs_num):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(epochs_num):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run BidirectionalRNN...\n",
      "Loading Glove vectors...\n",
      "start embedding...\n",
      "WARNING:tensorflow:From <ipython-input-16-779eeae90efd>:37: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "In step 100, The loss is: 2.8512744903564453\n",
      "\n",
      " Accuracy = 0.308805910543131\n",
      "\n",
      "In step 200, The loss is: 2.5423879623413086\n",
      "\n",
      " Accuracy = 0.35193690095846647\n",
      "\n",
      "In step 300, The loss is: 2.3322501182556152\n",
      "\n",
      " Accuracy = 0.3878294728434505\n",
      "\n",
      "In step 400, The loss is: 2.136857509613037\n",
      "\n",
      " Accuracy = 0.4020899893186344\n",
      "\n",
      "In step 500, The loss is: 1.9058177471160889\n",
      "\n",
      " Accuracy = 0.4197117944304555\n",
      "\n",
      "In step 600, The loss is: 2.4125397205352783\n",
      "\n",
      " Accuracy = 0.450063232130136\n",
      "\n",
      "In step 700, The loss is: 2.0428404808044434\n",
      "\n",
      " Accuracy = 0.44866546855186124\n",
      "\n",
      "In step 800, The loss is: 2.4209251403808594\n",
      "\n",
      " Accuracy = 0.4642738285727394\n",
      "\n",
      "In step 900, The loss is: 2.0842390060424805\n",
      "\n",
      " Accuracy = 0.46315894568690097\n",
      "\n",
      "In step 1000, The loss is: 2.21349835395813\n",
      "\n",
      " Accuracy = 0.4770367412140575\n",
      "\n",
      "In step 1100, The loss is: 1.6595337390899658\n",
      "\n",
      " Accuracy = 0.4850572418100156\n",
      "\n",
      "In step 1200, The loss is: 2.1741318702697754\n",
      "\n",
      " Accuracy = 0.4854898827906234\n",
      "\n",
      "In step 1300, The loss is: 2.2471985816955566\n",
      "\n",
      " Accuracy = 0.49469182647455234\n",
      "\n",
      "In step 1400, The loss is: 1.4914418458938599\n",
      "\n",
      " Accuracy = 0.496305910543131\n",
      "\n",
      "In step 1500, The loss is: 1.8284122943878174\n",
      "\n",
      " Accuracy = 0.4983027156549521\n",
      "\n",
      "In step 1600, The loss is: 1.932499647140503\n",
      "\n",
      " Accuracy = 0.5027289670496322\n",
      "\n",
      "In step 1700, The loss is: 1.6806316375732422\n",
      "\n",
      " Accuracy = 0.5121638712029868\n",
      "\n",
      "In step 1800, The loss is: 1.9793107509613037\n",
      "\n",
      " Accuracy = 0.5150592386151274\n",
      "\n",
      "In step 1900, The loss is: 1.7800133228302002\n",
      "\n",
      " Accuracy = 0.5159910808736905\n",
      "\n",
      "In step 2000, The loss is: 2.3144707679748535\n",
      "\n",
      " Accuracy = 0.5140608360592168\n",
      "\n",
      "In step 2100, The loss is: 1.9647927284240723\n",
      "\n",
      " Accuracy = 0.5138445153784829\n",
      "\n",
      "In step 2200, The loss is: 1.8954719305038452\n",
      "\n",
      " Accuracy = 0.519851570883498\n",
      "\n",
      "In step 2300, The loss is: 1.9062440395355225\n",
      "\n",
      " Accuracy = 0.5237287006819972\n",
      "\n",
      "In step 2400, The loss is: 1.5884966850280762\n",
      "\n",
      " Accuracy = 0.524377662343339\n",
      "\n",
      "In step 2500, The loss is: 1.5007984638214111\n",
      "\n",
      " Accuracy = 0.5265741479663423\n",
      "\n",
      "In step 2600, The loss is: 1.5871686935424805\n",
      "\n",
      " Accuracy = 0.528071751800208\n",
      "\n",
      "In step 2700, The loss is: 1.5255422592163086\n",
      "\n",
      " Accuracy = 0.5291699946117097\n",
      "\n",
      "In step 2800, The loss is: 2.0123424530029297\n",
      "\n",
      " Accuracy = 0.5287706335893454\n",
      "\n",
      "In step 2900, The loss is: 1.4032847881317139\n",
      "\n",
      " Accuracy = 0.5334797657716769\n",
      "\n",
      "In step 3000, The loss is: 1.7097865343093872\n",
      "\n",
      " Accuracy = 0.5319156016404636\n",
      "\n",
      "In step 3100, The loss is: 1.8402462005615234\n",
      "\n",
      " Accuracy = 0.5364416932907349\n",
      "\n",
      "In step 3200, The loss is: 1.4617063999176025\n",
      "\n",
      " Accuracy = 0.5340788073052233\n",
      "\n",
      "In step 3300, The loss is: 1.834947109222412\n",
      "\n",
      " Accuracy = 0.5349607294359908\n",
      "\n",
      "In step 3400, The loss is: 1.4062501192092896\n",
      "\n",
      " Accuracy = 0.5367911341853036\n",
      "\n",
      "In step 3500, The loss is: 1.7999180555343628\n",
      "\n",
      " Accuracy = 0.5340621671356713\n",
      "\n",
      "In step 3600, The loss is: 1.8931901454925537\n",
      "\n",
      " Accuracy = 0.5356928913738019\n",
      "\n",
      "In step 3700, The loss is: 1.493342399597168\n",
      "\n",
      " Accuracy = 0.5372404153354633\n",
      "\n",
      "In step 3800, The loss is: 1.6263706684112549\n",
      "\n",
      " Accuracy = 0.5380890575079872\n",
      "\n",
      "In step 3900, The loss is: 1.6323446035385132\n",
      "\n",
      " Accuracy = 0.5358592917363103\n",
      "\n",
      "In step 4000, The loss is: 1.5349392890930176\n",
      "\n",
      " Accuracy = 0.5350106495637863\n",
      "\n",
      "In step 4100, The loss is: 1.4039387702941895\n",
      "\n",
      " Accuracy = 0.5369076144200163\n",
      "\n",
      "In step 4200, The loss is: 1.4262090921401978\n",
      "\n",
      " Accuracy = 0.5381389776357828\n",
      "\n",
      "In step 4300, The loss is: 1.2676942348480225\n",
      "\n",
      " Accuracy = 0.5368576942922209\n",
      "\n",
      "In step 4400, The loss is: 1.5514237880706787\n",
      "\n",
      " Accuracy = 0.5380391373801917\n",
      "\n",
      "In step 4500, The loss is: 1.8612841367721558\n",
      "\n",
      " Accuracy = 0.5402189830240731\n",
      "\n",
      "In step 4600, The loss is: 1.6379562616348267\n",
      "\n",
      " Accuracy = 0.5424820287539937\n",
      "\n",
      "In step 4700, The loss is: 1.4861979484558105\n",
      "\n",
      " Accuracy = 0.5440129127365332\n",
      "\n",
      "In step 4800, The loss is: 1.5089749097824097\n",
      "\n",
      " Accuracy = 0.5402356230031949\n",
      "\n",
      "In step 4900, The loss is: 1.2989935874938965\n",
      "\n",
      " Accuracy = 0.538970979829185\n",
      "\n",
      "In step 5000, The loss is: 1.282710313796997\n",
      "\n",
      " Accuracy = 0.5424820287539937\n",
      "\n",
      "In step 5100, The loss is: 1.5986968278884888\n",
      "\n",
      " Accuracy = 0.5431309904153354\n",
      "\n",
      "In step 5200, The loss is: 1.225884199142456\n",
      "\n",
      " Accuracy = 0.5451277955271565\n",
      "\n",
      "In step 5300, The loss is: 1.4824554920196533\n",
      "\n",
      " Accuracy = 0.5425818690095847\n",
      "\n",
      "In step 5400, The loss is: 1.3836588859558105\n",
      "\n",
      " Accuracy = 0.5429313099041534\n",
      "\n",
      "In step 5500, The loss is: 1.7279765605926514\n",
      "\n",
      " Accuracy = 0.5423821884984026\n",
      "\n",
      "In step 5600, The loss is: 1.2286350727081299\n",
      "\n",
      " Accuracy = 0.5430977102666618\n",
      "\n",
      "In step 5700, The loss is: 1.869660496711731\n",
      "\n",
      " Accuracy = 0.5453441160174605\n",
      "\n",
      "In step 5800, The loss is: 1.4514586925506592\n",
      "\n",
      " Accuracy = 0.5441793130990416\n",
      "\n",
      "In step 5900, The loss is: 1.413182020187378\n",
      "\n",
      " Accuracy = 0.5422823482428115\n",
      "\n",
      "In step 6000, The loss is: 1.4503430128097534\n",
      "\n",
      " Accuracy = 0.5422324281150159\n",
      "\n",
      "In step 6100, The loss is: 1.4826602935791016\n",
      "\n",
      " Accuracy = 0.5413338658146964\n",
      "\n",
      "In step 6200, The loss is: 1.1871742010116577\n",
      "\n",
      " Accuracy = 0.5415169063467569\n",
      "\n",
      "In step 6300, The loss is: 1.2918040752410889\n",
      "\n",
      " Accuracy = 0.5410509850270451\n",
      "\n",
      "In step 6400, The loss is: 1.471285343170166\n",
      "\n",
      " Accuracy = 0.5434804313099042\n",
      "\n",
      "In step 6500, The loss is: 1.2628514766693115\n",
      "\n",
      " Accuracy = 0.5421658680080986\n",
      "\n",
      "In step 6600, The loss is: 1.5197521448135376\n",
      "\n",
      " Accuracy = 0.543180910543131\n",
      "\n",
      "In step 6700, The loss is: 1.412244200706482\n",
      "\n",
      " Accuracy = 0.5393703408515491\n",
      "\n",
      "In step 6800, The loss is: 1.5338366031646729\n",
      "\n",
      " Accuracy = 0.5411674654521881\n",
      "\n",
      "In step 6900, The loss is: 1.471052646636963\n",
      "\n",
      " Accuracy = 0.5387213791902072\n",
      "\n",
      "In step 7000, The loss is: 1.2838785648345947\n",
      "\n",
      " Accuracy = 0.5379892172523961\n",
      "\n",
      "In step 7100, The loss is: 1.4867160320281982\n",
      "\n",
      " Accuracy = 0.5401857028753994\n",
      "\n",
      "In step 7200, The loss is: 1.4891486167907715\n",
      "\n",
      " Accuracy = 0.5341786475608143\n",
      "\n",
      "In step 7300, The loss is: 1.4089641571044922\n",
      "\n",
      " Accuracy = 0.5438631523531466\n",
      "\n",
      "In step 7400, The loss is: 1.5542714595794678\n",
      "\n",
      " Accuracy = 0.5440129127365332\n",
      "\n",
      "In step 7500, The loss is: 1.333020806312561\n",
      "\n",
      " Accuracy = 0.5428314696485623\n",
      "\n",
      "In step 7600, The loss is: 1.424140214920044\n",
      "\n",
      " Accuracy = 0.5444122737588973\n",
      "\n",
      "In step 7700, The loss is: 1.6241803169250488\n",
      "\n",
      " Accuracy = 0.5377728967620923\n",
      "\n",
      "In step 7800, The loss is: 1.5232717990875244\n",
      "\n",
      " Accuracy = 0.5428813897763578\n",
      "\n",
      "In step 7900, The loss is: 1.6295497417449951\n",
      "\n",
      " Accuracy = 0.54497803514377\n",
      "\n",
      "In step 8000, The loss is: 1.128300428390503\n",
      "\n",
      " Accuracy = 0.5381389776357828\n",
      "\n",
      "In step 8100, The loss is: 1.2009084224700928\n",
      "\n",
      " Accuracy = 0.5384884185303515\n",
      "\n",
      "In step 8200, The loss is: 1.0223404169082642\n",
      "\n",
      " Accuracy = 0.5411175453243926\n",
      "\n",
      "In step 8300, The loss is: 1.4215269088745117\n",
      "\n",
      " Accuracy = 0.5374567358257671\n",
      "\n",
      "In step 8400, The loss is: 1.2506544589996338\n",
      "\n",
      " Accuracy = 0.5400359424920128\n",
      "\n",
      "In step 8500, The loss is: 1.4019970893859863\n",
      "\n",
      " Accuracy = 0.5390375399361023\n",
      "\n",
      "In step 8600, The loss is: 1.194547414779663\n",
      "\n",
      " Accuracy = 0.5422989882219332\n",
      "\n",
      "In step 8700, The loss is: 1.2497122287750244\n",
      "\n",
      " Accuracy = 0.5379892172523961\n",
      "\n",
      "In step 8800, The loss is: 1.0378878116607666\n",
      "\n",
      " Accuracy = 0.5356928913738019\n",
      "\n",
      "In step 8900, The loss is: 1.3269338607788086\n",
      "\n",
      " Accuracy = 0.5346112885414221\n",
      "\n",
      "In step 9000, The loss is: 1.0359665155410767\n",
      "\n",
      " Accuracy = 0.5359424920127795\n",
      "\n",
      "In step 9100, The loss is: 1.6215229034423828\n",
      "\n",
      " Accuracy = 0.5315328807876514\n",
      "\n",
      "In step 9200, The loss is: 1.8486227989196777\n",
      "\n",
      " Accuracy = 0.5328474440894568\n",
      "\n",
      "In step 9300, The loss is: 1.1313329935073853\n",
      "\n",
      " Accuracy = 0.534095447284345\n",
      "\n",
      "In step 9400, The loss is: 1.1087111234664917\n",
      "\n",
      " Accuracy = 0.5330970447284346\n",
      "\n",
      "In step 9500, The loss is: 1.2839235067367554\n",
      "\n",
      " Accuracy = 0.5295027955271565\n",
      "\n",
      "In step 9600, The loss is: 1.1842877864837646\n",
      "\n",
      " Accuracy = 0.5351271299889293\n",
      "\n",
      "In step 9700, The loss is: 1.0945487022399902\n",
      "\n",
      " Accuracy = 0.5347277689665652\n",
      "\n",
      "In step 9800, The loss is: 0.7978641986846924\n",
      "\n",
      " Accuracy = 0.5305511182108626\n",
      "\n",
      "In step 9900, The loss is: 1.1661736965179443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy = 0.5321319223211977\n",
      "\n",
      "In step 10000, The loss is: 0.9677817821502686\n",
      "\n",
      " Accuracy = 0.5340288871774277\n",
      "\n",
      "In step 10100, The loss is: 1.2003710269927979\n",
      "\n",
      " Accuracy = 0.5303347977205587\n",
      "\n",
      "In step 10200, The loss is: 1.0953617095947266\n",
      "\n",
      " Accuracy = 0.528404552715655\n",
      "\n",
      "In step 10300, The loss is: 1.4708642959594727\n",
      "\n",
      " Accuracy = 0.5294362354202392\n",
      "\n",
      "In step 10400, The loss is: 0.788610577583313\n",
      "\n",
      " Accuracy = 0.5341287274330188\n",
      "\n",
      "In step 10500, The loss is: 1.289271593093872\n",
      "\n",
      " Accuracy = 0.5310503194888179\n",
      "\n",
      "In step 10600, The loss is: 1.114904522895813\n",
      "\n",
      " Accuracy = 0.5329472843450479\n",
      "\n",
      "In step 10700, The loss is: 1.0345767736434937\n",
      "\n",
      " Accuracy = 0.5307008785942492\n",
      "\n",
      "In step 10800, The loss is: 1.180433750152588\n",
      "\n",
      " Accuracy = 0.5284378328643287\n",
      "\n",
      "In step 10900, The loss is: 0.8260524272918701\n",
      "\n",
      " Accuracy = 0.5274727102666618\n",
      "\n",
      "In step 11000, The loss is: 1.2465569972991943\n",
      "\n",
      " Accuracy = 0.5275392705640092\n",
      "\n",
      "In step 11100, The loss is: 1.0116686820983887\n",
      "\n",
      " Accuracy = 0.5314330405320603\n",
      "\n",
      "In step 11200, The loss is: 0.955026388168335\n",
      "\n",
      " Accuracy = 0.5195187699680511\n",
      "\n",
      "In step 11300, The loss is: 1.1872453689575195\n",
      "\n",
      " Accuracy = 0.5241114217252396\n",
      "\n",
      "In step 11400, The loss is: 1.4137146472930908\n",
      "\n",
      " Accuracy = 0.5323149626628279\n",
      "\n",
      "In step 11500, The loss is: 1.8660500049591064\n",
      "\n",
      " Accuracy = 0.5275725505222528\n",
      "\n",
      "In step 11600, The loss is: 0.9795323610305786\n",
      "\n",
      " Accuracy = 0.5311834398930827\n",
      "\n",
      "In step 11700, The loss is: 0.8252695202827454\n",
      "\n",
      " Accuracy = 0.5271232693720931\n",
      "\n",
      "In step 11800, The loss is: 1.0650551319122314\n",
      "\n",
      " Accuracy = 0.525242944685415\n",
      "\n",
      "In step 11900, The loss is: 0.8228031396865845\n",
      "\n",
      " Accuracy = 0.5267904686470763\n",
      "\n",
      "In step 12000, The loss is: 1.2367384433746338\n",
      "\n",
      " Accuracy = 0.5228634185303515\n",
      "\n",
      "In step 12100, The loss is: 0.7128501534461975\n",
      "\n",
      " Accuracy = 0.5253095047923323\n",
      "\n",
      "In step 12200, The loss is: 1.155430793762207\n",
      "\n",
      " Accuracy = 0.5251597444089456\n",
      "\n",
      "In step 12300, The loss is: 0.5977418422698975\n",
      "\n",
      " Accuracy = 0.5238784610653837\n",
      "\n",
      "In step 12400, The loss is: 0.9793442487716675\n",
      "\n",
      " Accuracy = 0.5204173322683706\n",
      "\n",
      "In step 12500, The loss is: 1.2113317251205444\n",
      "\n",
      " Accuracy = 0.5223309371037225\n",
      "\n",
      "In step 12600, The loss is: 1.3790240287780762\n",
      "\n",
      " Accuracy = 0.5241114217252396\n",
      "\n",
      "In step 12700, The loss is: 1.264897346496582\n",
      "\n",
      " Accuracy = 0.5186701277955271\n",
      "\n",
      "In step 12800, The loss is: 0.797754168510437\n",
      "\n",
      " Accuracy = 0.5223309371037225\n",
      "\n",
      "In step 12900, The loss is: 1.2781388759613037\n",
      "\n",
      " Accuracy = 0.5229466188068207\n",
      "\n",
      "In step 13000, The loss is: 0.9453527331352234\n",
      "\n",
      " Accuracy = 0.5178214856230032\n",
      "\n",
      "In step 13100, The loss is: 1.1049957275390625\n",
      "\n",
      " Accuracy = 0.5240448616183223\n",
      "\n",
      "In step 13200, The loss is: 0.8466349840164185\n",
      "\n",
      " Accuracy = 0.5199014910112936\n",
      "\n",
      "In step 13300, The loss is: 0.9337872862815857\n",
      "\n",
      " Accuracy = 0.5223642172523961\n",
      "\n",
      "In step 13400, The loss is: 1.1572672128677368\n",
      "\n",
      " Accuracy = 0.5184205271565495\n",
      "\n",
      "In step 13500, The loss is: 0.8431348204612732\n",
      "\n",
      " Accuracy = 0.5180211661341853\n",
      "\n",
      "In step 13600, The loss is: 0.6803144216537476\n",
      "\n",
      " Accuracy = 0.5170227635782748\n",
      "\n",
      "In step 13700, The loss is: 1.314985990524292\n",
      "\n",
      " Accuracy = 0.5208166932907349\n",
      "\n",
      "In step 13800, The loss is: 1.1874089241027832\n",
      "\n",
      " Accuracy = 0.5237453408515491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = BidirectionalRNN(reversed_dictionary, doc_max_len, num_class)\n",
    "\n",
    "    train_batches = batch_iter(train_x, train_y, 64, 10)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // 64 + 1\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    for x_batch, y_batch in train_batches:\n",
    "        train_feed_dict = {\n",
    "            model.x: x_batch,\n",
    "            model.y: y_batch,\n",
    "            model.keep_prob: 0.8\n",
    "        }\n",
    "        _, step, loss = sess.run([model.optimizer, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"In step {0}, The loss is: {1}\".format(step, loss))\n",
    "        if step % 100 == 0:\n",
    "            valid_batches = batch_iter(valid_x, valid_y, 64, 1)\n",
    "            sum_accuracy, count = 0, 0\n",
    "            for valid_x_batch, valid_y_batch in valid_batches:\n",
    "                valid_feed_dict = {\n",
    "                    model.x: valid_x_batch,\n",
    "                    model.y: valid_y_batch,\n",
    "                    model.keep_prob: 1.0\n",
    "                }\n",
    "                accuracy = sess.run(model.accuracy, feed_dict=valid_feed_dict)\n",
    "                sum_accuracy += accuracy\n",
    "                count += 1\n",
    "            temp_accuracy = sum_accuracy / count\n",
    "\n",
    "            print(\"\\n Accuracy = {1}\\n\".format(step // num_batches_per_epoch, sum_accuracy / count))\n",
    "            # Save the model that can produce the best accuracy\n",
    "            if temp_accuracy > best_accuracy:\n",
    "                best_accuracy = temp_accuracy\n",
    "                model_saver.save(sess, \"{0}/{1}.ckpt\".format(\"saved_model\", \"naive\"), global_step=step)\n",
    "    \n",
    "    print(\"end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNN(object):\n",
    "    def __init__(self, reversed_dict, document_max_len, num_class):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = 300\n",
    "        self.num_hidden = 100\n",
    "        self.num_layers = 2\n",
    "        self.learning_rate = 1e-3\n",
    "\n",
    "        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=\"x\")\n",
    "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
    "        self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"keep_prob\")\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size])\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings, trainable=True)\n",
    "            self.x_emb = tf.nn.embedding_lookup(self.embeddings, self.x)\n",
    "\n",
    "        with tf.name_scope(\"birnn\"):\n",
    "            fw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "            self.rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
    "\n",
    "        with tf.name_scope(\"attention\"):\n",
    "            self.attention_score = tf.nn.softmax(tf.contrib.slim.fully_connected(self.rnn_outputs, 1))\n",
    "            self.attention_out = tf.squeeze(\n",
    "                tf.matmul(tf.transpose(self.rnn_outputs, perm=[0, 2, 1]), self.attention_score),\n",
    "                axis=-1)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.logits = tf.contrib.slim.fully_connected(self.attention_out, num_class, activation_fn=None)\n",
    "            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, self.y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-cbd5cdd63f57>:21: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "In step 100, The loss is: 3.4723939895629883\n",
      "\n",
      " Accuracy = 0.16480298190356824\n",
      "\n",
      "In step 200, The loss is: 3.070613384246826\n",
      "\n",
      " Accuracy = 0.2612153886796567\n",
      "\n",
      "In step 300, The loss is: 2.9416391849517822\n",
      "\n",
      " Accuracy = 0.2800685570072442\n",
      "\n",
      "In step 400, The loss is: 2.58505916595459\n",
      "\n",
      " Accuracy = 0.316676650708095\n",
      "\n",
      "In step 500, The loss is: 2.8270888328552246\n",
      "\n",
      " Accuracy = 0.3687433439512222\n",
      "\n",
      "In step 600, The loss is: 2.4755616188049316\n",
      "\n",
      " Accuracy = 0.3902089989032989\n",
      "\n",
      "In step 700, The loss is: 2.3452911376953125\n",
      "\n",
      " Accuracy = 0.4193956336845605\n",
      "\n",
      "In step 800, The loss is: 2.3340704441070557\n",
      "\n",
      " Accuracy = 0.4456536209050078\n",
      "\n",
      "In step 900, The loss is: 1.7437608242034912\n",
      "\n",
      " Accuracy = 0.42888045796571067\n",
      "\n",
      "In step 1000, The loss is: 1.8474345207214355\n",
      "\n",
      " Accuracy = 0.4615448615231072\n",
      "\n",
      "In step 1100, The loss is: 2.090970993041992\n",
      "\n",
      " Accuracy = 0.4899327742596404\n",
      "\n",
      "In step 1200, The loss is: 1.7409323453903198\n",
      "\n",
      " Accuracy = 0.4887679712460064\n",
      "\n",
      "In step 1300, The loss is: 2.3380684852600098\n",
      "\n",
      " Accuracy = 0.5007321618425961\n",
      "\n",
      "In step 1400, The loss is: 1.9068686962127686\n",
      "\n",
      " Accuracy = 0.5126297923322684\n",
      "\n",
      "In step 1500, The loss is: 1.8470845222473145\n",
      "\n",
      " Accuracy = 0.5139609957084107\n",
      "\n",
      "In step 1600, The loss is: 1.925694227218628\n",
      "\n",
      " Accuracy = 0.5161242012779552\n",
      "\n",
      "In step 1700, The loss is: 2.0670924186706543\n",
      "\n",
      " Accuracy = 0.5347444089456869\n",
      "\n",
      "In step 1800, The loss is: 1.9153127670288086\n",
      "\n",
      " Accuracy = 0.5319488817891374\n",
      "\n",
      "In step 1900, The loss is: 1.689636468887329\n",
      "\n",
      " Accuracy = 0.5343616879976596\n",
      "\n",
      "In step 2000, The loss is: 1.6648370027542114\n",
      "\n",
      " Accuracy = 0.5420493876781708\n",
      "\n",
      "In step 2100, The loss is: 1.6620599031448364\n",
      "\n",
      " Accuracy = 0.5477402822468609\n",
      "\n",
      "In step 2200, The loss is: 1.4385435581207275\n",
      "\n",
      " Accuracy = 0.5451943556340738\n",
      "\n",
      "In step 2300, The loss is: 1.6473634243011475\n",
      "\n",
      " Accuracy = 0.5404852236421726\n",
      "\n",
      "In step 2400, The loss is: 1.8423550128936768\n",
      "\n",
      " Accuracy = 0.5651624067522847\n",
      "\n",
      "In step 2500, The loss is: 2.054692268371582\n",
      "\n",
      " Accuracy = 0.5537306975823241\n",
      "\n",
      "In step 2600, The loss is: 1.7265619039535522\n",
      "\n",
      " Accuracy = 0.5624500798722045\n",
      "\n",
      "In step 2700, The loss is: 1.560815453529358\n",
      "\n",
      " Accuracy = 0.5667598508417416\n",
      "\n",
      "In step 2800, The loss is: 1.9276611804962158\n",
      "\n",
      " Accuracy = 0.5686401757188498\n",
      "\n",
      "In step 2900, The loss is: 1.7190942764282227\n",
      "\n",
      " Accuracy = 0.5652622471030909\n",
      "\n",
      "In step 3000, The loss is: 1.552612066268921\n",
      "\n",
      " Accuracy = 0.5681076943874359\n",
      "\n",
      "In step 3100, The loss is: 1.521579623222351\n",
      "\n",
      " Accuracy = 0.5631489616613419\n",
      "\n",
      "In step 3200, The loss is: 1.5658833980560303\n",
      "\n",
      " Accuracy = 0.571152822087748\n",
      "\n",
      "In step 3300, The loss is: 2.048597812652588\n",
      "\n",
      " Accuracy = 0.5741646699250316\n",
      "\n",
      "In step 3400, The loss is: 1.504381537437439\n",
      "\n",
      " Accuracy = 0.5844482162509101\n",
      "\n",
      "In step 3500, The loss is: 1.2420607805252075\n",
      "\n",
      " Accuracy = 0.5648795260598485\n",
      "\n",
      "In step 3600, The loss is: 1.3627269268035889\n",
      "\n",
      " Accuracy = 0.5817858093081952\n",
      "\n",
      "In step 3700, The loss is: 1.5306568145751953\n",
      "\n",
      " Accuracy = 0.5777755591054313\n",
      "\n",
      "In step 3800, The loss is: 1.582977533340454\n",
      "\n",
      " Accuracy = 0.5799387645797608\n",
      "\n",
      "In step 3900, The loss is: 1.432385802268982\n",
      "\n",
      " Accuracy = 0.5881423057077791\n",
      "\n",
      "In step 4000, The loss is: 1.358026146888733\n",
      "\n",
      " Accuracy = 0.5940994408945687\n",
      "\n",
      "In step 4100, The loss is: 1.5092782974243164\n",
      "\n",
      " Accuracy = 0.575695553812356\n",
      "\n",
      "In step 4200, The loss is: 1.3390986919403076\n",
      "\n",
      " Accuracy = 0.5860123801916933\n",
      "\n",
      "In step 4300, The loss is: 1.1472399234771729\n",
      "\n",
      " Accuracy = 0.5955637645797608\n",
      "\n",
      "In step 4400, The loss is: 1.5427961349487305\n",
      "\n",
      " Accuracy = 0.5894402290304629\n",
      "\n",
      "In step 4500, The loss is: 1.1899350881576538\n",
      "\n",
      " Accuracy = 0.5880923855799837\n",
      "\n",
      "In step 4600, The loss is: 1.0579403638839722\n",
      "\n",
      " Accuracy = 0.5802216453674122\n",
      "\n",
      "In step 4700, The loss is: 1.2727715969085693\n",
      "\n",
      " Accuracy = 0.5868776624385541\n",
      "\n",
      "In step 4800, The loss is: 1.6561170816421509\n",
      "\n",
      " Accuracy = 0.5838824546756074\n",
      "\n",
      "In step 4900, The loss is: 1.292801856994629\n",
      "\n",
      " Accuracy = 0.5877762247388736\n",
      "\n",
      "In step 5000, The loss is: 1.4514057636260986\n",
      "\n",
      " Accuracy = 0.5921525559105432\n",
      "\n",
      "In step 5100, The loss is: 1.380551815032959\n",
      "\n",
      " Accuracy = 0.6003061767203358\n",
      "\n",
      "In step 5200, The loss is: 1.0210611820220947\n",
      "\n",
      " Accuracy = 0.5852302981260866\n",
      "\n",
      "In step 5300, The loss is: 1.3784925937652588\n",
      "\n",
      " Accuracy = 0.6028853833865815\n",
      "\n",
      "In step 5400, The loss is: 1.1834170818328857\n",
      "\n",
      " Accuracy = 0.5947484025559105\n",
      "\n",
      "In step 5500, The loss is: 1.41994309425354\n",
      "\n",
      " Accuracy = 0.5946485623003195\n",
      "\n",
      "In step 5600, The loss is: 1.1198817491531372\n",
      "\n",
      " Accuracy = 0.5937167199465413\n",
      "\n",
      "In step 5700, The loss is: 1.2858848571777344\n",
      "\n",
      " Accuracy = 0.5993743344617728\n",
      "\n",
      "In step 5800, The loss is: 1.04256010055542\n",
      "\n",
      " Accuracy = 0.597860090458355\n",
      "\n",
      "In step 5900, The loss is: 1.315242886543274\n",
      "\n",
      " Accuracy = 0.6028188232796642\n",
      "\n",
      "In step 6000, The loss is: 1.0459667444229126\n",
      "\n",
      " Accuracy = 0.5888911076247121\n",
      "\n",
      "In step 6100, The loss is: 0.9958220720291138\n",
      "\n",
      " Accuracy = 0.6018869808306709\n",
      "\n",
      "In step 6200, The loss is: 1.4214842319488525\n",
      "\n",
      " Accuracy = 0.599773695484137\n",
      "\n",
      "In step 6300, The loss is: 0.9169689416885376\n",
      "\n",
      " Accuracy = 0.5822517306279069\n",
      "\n",
      "In step 6400, The loss is: 0.9051378965377808\n",
      "\n",
      " Accuracy = 0.5939164003625084\n",
      "\n",
      "In step 6500, The loss is: 1.2243826389312744\n",
      "\n",
      " Accuracy = 0.5874933440464373\n",
      "\n",
      "In step 6600, The loss is: 0.9129461050033569\n",
      "\n",
      " Accuracy = 0.5877595846645367\n",
      "\n",
      "In step 6700, The loss is: 1.4117794036865234\n",
      "\n",
      " Accuracy = 0.594282481426629\n",
      "\n",
      "In step 6800, The loss is: 0.8303778171539307\n",
      "\n",
      " Accuracy = 0.5968284079442009\n",
      "\n",
      "In step 6900, The loss is: 1.0710091590881348\n",
      "\n",
      " Accuracy = 0.5957634450909429\n",
      "\n",
      "In step 7000, The loss is: 0.8473336100578308\n",
      "\n",
      " Accuracy = 0.5933672789567576\n",
      "\n",
      "In step 7100, The loss is: 1.1295279264450073\n",
      "\n",
      " Accuracy = 0.5989583332698566\n",
      "\n",
      "In step 7200, The loss is: 0.9810906648635864\n",
      "\n",
      " Accuracy = 0.5859957402125715\n",
      "\n",
      "In step 7300, The loss is: 1.1411412954330444\n",
      "\n",
      " Accuracy = 0.589540069286054\n",
      "\n",
      "In step 7400, The loss is: 0.8491026759147644\n",
      "\n",
      " Accuracy = 0.5945487220447284\n",
      "\n",
      "In step 7500, The loss is: 1.0200374126434326\n",
      "\n",
      " Accuracy = 0.599341054313099\n",
      "\n",
      "In step 7600, The loss is: 1.0150527954101562\n",
      "\n",
      " Accuracy = 0.5918863151972287\n",
      "\n",
      "In step 7700, The loss is: 1.274972677230835\n",
      "\n",
      " Accuracy = 0.6014876198083067\n",
      "\n",
      "In step 7800, The loss is: 0.7389448881149292\n",
      "\n",
      " Accuracy = 0.5982261714272605\n",
      "\n",
      "In step 7900, The loss is: 0.9267711639404297\n",
      "\n",
      " Accuracy = 0.5977935303514377\n",
      "\n",
      "In step 8000, The loss is: 1.265982747077942\n",
      "\n",
      " Accuracy = 0.5909378328643287\n",
      "\n",
      "In step 8100, The loss is: 1.1497431993484497\n",
      "\n",
      " Accuracy = 0.592385516570399\n",
      "\n",
      "In step 8200, The loss is: 1.0614556074142456\n",
      "\n",
      " Accuracy = 0.5987253728004309\n",
      "\n",
      "In step 8300, The loss is: 0.9017592668533325\n",
      "\n",
      " Accuracy = 0.601570820084776\n",
      "\n",
      "In step 8400, The loss is: 1.2040857076644897\n",
      "\n",
      " Accuracy = 0.5894402290304629\n",
      "\n",
      "In step 8500, The loss is: 0.9317604303359985\n",
      "\n",
      " Accuracy = 0.5861122204472844\n",
      "\n",
      "In step 8600, The loss is: 0.7799069881439209\n",
      "\n",
      " Accuracy = 0.5946818424489932\n",
      "\n",
      "In step 8700, The loss is: 0.6380233764648438\n",
      "In step 9300, The loss is: 1.0611841678619385\n",
      "\n",
      " Accuracy = 0.5815694888178914\n",
      "\n",
      "In step 9400, The loss is: 0.8430466055870056\n",
      "\n",
      " Accuracy = 0.5930011980830671\n",
      "\n",
      "In step 9500, The loss is: 0.7595065832138062\n",
      "\n",
      " Accuracy = 0.5973109690526042\n",
      "\n",
      "In step 9600, The loss is: 0.559950590133667\n",
      "\n",
      " Accuracy = 0.5928680776788023\n",
      "\n",
      "In step 9700, The loss is: 1.0730092525482178\n",
      "\n",
      " Accuracy = 0.594282481426629\n",
      "\n",
      "In step 9800, The loss is: 0.737824559211731\n",
      "\n",
      " Accuracy = 0.5983592917363103\n",
      "\n",
      "In step 9900, The loss is: 0.8398109674453735\n",
      "\n",
      " Accuracy = 0.5946818424489932\n",
      "\n",
      "In step 10000, The loss is: 0.8473979234695435\n",
      "\n",
      " Accuracy = 0.5999068156979717\n",
      "\n",
      "In step 10100, The loss is: 1.0053083896636963\n",
      "\n",
      " Accuracy = 0.5814363685088416\n",
      "\n",
      "In step 10200, The loss is: 0.6702897548675537\n",
      "\n",
      " Accuracy = 0.5793230830670927\n",
      "\n",
      "In step 10300, The loss is: 1.307689905166626\n",
      "\n",
      " Accuracy = 0.5919195953459023\n",
      "\n",
      "In step 10400, The loss is: 0.7650099992752075\n",
      "\n",
      " Accuracy = 0.5880591054313099\n",
      "\n",
      "In step 10500, The loss is: 1.004077672958374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy = 0.5888744675503752\n",
      "\n",
      "In step 10600, The loss is: 0.6057698726654053\n",
      "\n",
      " Accuracy = 0.5882754260168289\n",
      "\n",
      "In step 10700, The loss is: 0.8657779693603516\n",
      "\n",
      " Accuracy = 0.5864616613418531\n",
      "\n",
      "In step 10800, The loss is: 0.8532254695892334\n",
      "\n",
      " Accuracy = 0.5856462993180028\n",
      "\n",
      "In step 10900, The loss is: 0.8311904072761536\n",
      "\n",
      " Accuracy = 0.5794728434504792\n",
      "\n",
      "In step 11000, The loss is: 0.6224357485771179\n",
      "\n",
      " Accuracy = 0.5931343183921168\n",
      "\n",
      "In step 11100, The loss is: 0.8200563788414001\n",
      "\n",
      " Accuracy = 0.5931343184873319\n",
      "\n",
      "In step 11200, The loss is: 0.7442889213562012\n",
      "\n",
      " Accuracy = 0.5877928648132105\n",
      "\n",
      "In step 11300, The loss is: 0.8108762502670288\n",
      "\n",
      " Accuracy = 0.5852302982213017\n",
      "\n",
      "In step 11400, The loss is: 0.8627128005027771\n",
      "\n",
      " Accuracy = 0.5709864217252396\n",
      "\n",
      "In step 11500, The loss is: 0.6276863813400269\n",
      "\n",
      " Accuracy = 0.5811701277955271\n",
      "\n",
      "In step 11600, The loss is: 0.6953887939453125\n",
      "\n",
      " Accuracy = 0.5806376463688981\n",
      "\n",
      "In step 11700, The loss is: 1.0666508674621582\n",
      "\n",
      " Accuracy = 0.5888744674551601\n",
      "\n",
      "In step 11800, The loss is: 0.7204011678695679\n",
      "\n",
      " Accuracy = 0.5919861555480348\n",
      "\n",
      "In step 11900, The loss is: 0.7229113578796387\n",
      "\n",
      " Accuracy = 0.5833166932907349\n",
      "\n",
      "In step 12000, The loss is: 0.47568273544311523\n",
      "\n",
      " Accuracy = 0.5795227635782748\n",
      "\n",
      "In step 12100, The loss is: 0.5308752059936523\n",
      "\n",
      " Accuracy = 0.583083732630879\n",
      "\n",
      "In step 12200, The loss is: 0.5475594997406006\n",
      "\n",
      " Accuracy = 0.5773928381574039\n",
      "\n",
      "In step 12300, The loss is: 0.7790668606758118\n",
      "\n",
      " Accuracy = 0.5741147497972361\n",
      "\n",
      "In step 12400, The loss is: 0.9681922197341919\n",
      "\n",
      " Accuracy = 0.5721845047923323\n",
      "\n",
      "In step 12500, The loss is: 0.6443193554878235\n",
      "\n",
      " Accuracy = 0.5782082002764692\n",
      "\n",
      "In step 12600, The loss is: 0.477815181016922\n",
      "\n",
      " Accuracy = 0.5795560437269485\n",
      "\n",
      "In step 12700, The loss is: 0.6112602949142456\n",
      "\n",
      " Accuracy = 0.5710197017786983\n",
      "\n",
      "In step 12800, The loss is: 0.8622041344642639\n",
      "\n",
      " Accuracy = 0.5835662939297125\n",
      "\n",
      "In step 12900, The loss is: 0.43848249316215515\n",
      "\n",
      " Accuracy = 0.5885915868579389\n",
      "\n",
      "In step 13000, The loss is: 0.6337378025054932\n",
      "\n",
      " Accuracy = 0.5748801916932907\n",
      "\n",
      "In step 13100, The loss is: 0.6604710817337036\n",
      "\n",
      " Accuracy = 0.5680078541318448\n",
      "\n",
      "In step 13200, The loss is: 0.695037841796875\n",
      "\n",
      " Accuracy = 0.5652788870822126\n",
      "\n",
      "In step 13300, The loss is: 0.8278033137321472\n",
      "\n",
      " Accuracy = 0.5757121937914779\n",
      "\n",
      "In step 13400, The loss is: 0.8076181411743164\n",
      "\n",
      " Accuracy = 0.5721512247388736\n",
      "\n",
      "In step 13500, The loss is: 0.7763881683349609\n",
      "\n",
      " Accuracy = 0.5692891373801917\n",
      "\n",
      "In step 13600, The loss is: 0.9272681474685669\n",
      "\n",
      " Accuracy = 0.5798222843450479\n",
      "\n",
      "In step 13700, The loss is: 0.6760460138320923\n",
      "\n",
      " Accuracy = 0.5799554046540977\n",
      "\n",
      "In step 13800, The loss is: 0.6327542066574097\n",
      "\n",
      " Accuracy = 0.5882920659959506\n",
      "\n",
      "In step 13900, The loss is: 0.6659854650497437\n",
      "\n",
      " Accuracy = 0.5830504525774203\n",
      "\n",
      "In step 14000, The loss is: 0.6489863395690918\n",
      "\n",
      " Accuracy = 0.5677249733441935\n",
      "\n",
      "In step 14100, The loss is: 0.43076613545417786\n",
      "\n",
      " Accuracy = 0.5839656549520766\n",
      "\n",
      "In step 14200, The loss is: 0.5534230470657349\n",
      "\n",
      " Accuracy = 0.5844315760813582\n",
      "\n",
      "In step 14300, The loss is: 0.588295578956604\n",
      "\n",
      " Accuracy = 0.579389643269225\n",
      "\n",
      "In step 14400, The loss is: 0.47512638568878174\n",
      "\n",
      " Accuracy = 0.5686734558675236\n",
      "\n",
      "In step 14500, The loss is: 0.5218542814254761\n",
      "\n",
      " Accuracy = 0.567159211959321\n",
      "\n",
      "In step 14600, The loss is: 0.5219375491142273\n",
      "\n",
      " Accuracy = 0.5817525293499517\n",
      "\n",
      "In step 14700, The loss is: 0.9232056140899658\n",
      "\n",
      " Accuracy = 0.571369142768482\n",
      "\n",
      "In step 14800, The loss is: 0.557071328163147\n",
      "\n",
      " Accuracy = 0.564180644270711\n",
      "\n",
      "In step 14900, The loss is: 0.5363767147064209\n",
      "\n",
      " Accuracy = 0.5681076943874359\n",
      "\n",
      "In step 15000, The loss is: 0.41781800985336304\n",
      "\n",
      " Accuracy = 0.5729166666349284\n",
      "\n",
      "In step 15100, The loss is: 0.5420524477958679\n",
      "\n",
      " Accuracy = 0.5721845047923323\n",
      "\n",
      "In step 15200, The loss is: 0.5263097286224365\n",
      "\n",
      " Accuracy = 0.5691060969433465\n",
      "\n",
      "In step 15300, The loss is: 0.48753854632377625\n",
      "\n",
      " Accuracy = 0.5416001065280109\n",
      "\n",
      "In step 15400, The loss is: 0.4285643696784973\n",
      "\n",
      " Accuracy = 0.5757121938866929\n",
      "\n",
      "In step 15500, The loss is: 0.3739197850227356\n",
      "\n",
      " Accuracy = 0.5702209797339698\n",
      "\n",
      "In step 15600, The loss is: 0.5546349287033081\n",
      "\n",
      " Accuracy = 0.5741813099041534\n",
      "\n",
      "In step 15700, The loss is: 0.571178138256073\n",
      "\n",
      " Accuracy = 0.5773595581039453\n",
      "\n",
      "In step 15800, The loss is: 0.3420313000679016\n",
      "\n",
      " Accuracy = 0.5536974174336503\n",
      "\n",
      "In step 15900, The loss is: 0.6646071672439575\n",
      "\n",
      " Accuracy = 0.5631156816078832\n",
      "\n",
      "In step 16000, The loss is: 0.2802799940109253\n",
      "\n",
      " Accuracy = 0.5699048189880749\n",
      "\n",
      "In step 16100, The loss is: 0.8154247999191284\n",
      "\n",
      " Accuracy = 0.5572916666349284\n",
      "\n",
      "In step 16200, The loss is: 0.8462380170822144\n",
      "\n",
      " Accuracy = 0.5673089723427075\n",
      "\n",
      "In step 16300, The loss is: 0.5689970850944519\n",
      "\n",
      " Accuracy = 0.5662606496590014\n",
      "\n",
      "In step 16400, The loss is: 0.47045522928237915\n",
      "\n",
      " Accuracy = 0.5648795261550635\n",
      "\n",
      "In step 16500, The loss is: 0.5480111241340637\n",
      "\n",
      " Accuracy = 0.5577243078059663\n",
      "\n",
      "In step 16600, The loss is: 0.5743201971054077\n",
      "\n",
      " Accuracy = 0.55902223112865\n",
      "\n",
      "In step 16700, The loss is: 0.6404463052749634\n",
      "\n",
      " Accuracy = 0.5649128062085221\n",
      "\n",
      "In step 16800, The loss is: 0.5579482316970825\n",
      "\n",
      " Accuracy = 0.5724507455056468\n",
      "\n",
      "In step 16900, The loss is: 0.42641332745552063\n",
      "\n",
      " Accuracy = 0.5594881522579315\n",
      "\n",
      "In step 17000, The loss is: 0.4649301767349243\n",
      "\n",
      " Accuracy = 0.5725838658146964\n",
      "\n",
      "In step 17100, The loss is: 0.9050273299217224\n",
      "\n",
      " Accuracy = 0.5690395367412141\n",
      "\n",
      "In step 17200, The loss is: 0.35096633434295654\n",
      "\n",
      " Accuracy = 0.555744142673267\n",
      "\n",
      "In step 17300, The loss is: 0.7105445265769958\n",
      "\n",
      " Accuracy = 0.5681908945686901\n",
      "\n",
      "In step 17400, The loss is: 0.43023133277893066\n",
      "\n",
      " Accuracy = 0.5605198349625158\n",
      "\n",
      "In step 17500, The loss is: 0.3669552803039551\n",
      "\n",
      " Accuracy = 0.5591719914168215\n",
      "\n",
      "In step 17600, The loss is: 0.4750076234340668\n",
      "\n",
      " Accuracy = 0.5631656017356788\n",
      "\n",
      "In step 17700, The loss is: 0.37935489416122437\n",
      "\n",
      " Accuracy = 0.5595047923322684\n",
      "\n",
      "In step 17800, The loss is: 0.42800575494766235\n",
      "\n",
      " Accuracy = 0.5563431842068133\n",
      "\n",
      "In step 17900, The loss is: 0.4683975279331207\n",
      "\n",
      " Accuracy = 0.5510017305326919\n",
      "\n",
      "In step 18000, The loss is: 0.5272400379180908\n",
      "\n",
      " Accuracy = 0.5699880191693291\n",
      "\n",
      "In step 18100, The loss is: 0.6281078457832336\n",
      "\n",
      " Accuracy = 0.5590055910543131\n",
      "\n",
      "In step 18200, The loss is: 0.38610392808914185\n",
      "\n",
      " Accuracy = 0.565778088455383\n",
      "\n",
      "In step 18300, The loss is: 0.26991143822669983\n",
      "\n",
      " Accuracy = 0.5520334132372762\n",
      "\n",
      "In step 18400, The loss is: 0.531167209148407\n",
      "\n",
      " Accuracy = 0.5649128061133071\n",
      "\n",
      "In step 18500, The loss is: 0.7612138390541077\n",
      "\n",
      " Accuracy = 0.5644302449096887\n",
      "\n",
      "In step 18600, The loss is: 0.49096885323524475\n",
      "\n",
      " Accuracy = 0.5543962992227878\n",
      "\n",
      "In step 18700, The loss is: 0.7062753438949585\n",
      "\n",
      " Accuracy = 0.5589390308521807\n",
      "\n",
      "In step 18800, The loss is: 0.4112980365753174\n",
      "\n",
      " Accuracy = 0.5456436368794487\n",
      "\n",
      "In step 18900, The loss is: 0.3285021483898163\n",
      "\n",
      " Accuracy = 0.5537639776357828\n",
      "\n",
      "In step 19000, The loss is: 0.497842937707901\n",
      "\n",
      " Accuracy = 0.5616180777740174\n",
      "\n",
      "In step 19100, The loss is: 0.5262001752853394\n",
      "\n",
      " Accuracy = 0.5594049520766773\n",
      "\n",
      "In step 19200, The loss is: 0.40645545721054077\n",
      "\n",
      " Accuracy = 0.5478068423537782\n",
      "\n",
      "In step 19300, The loss is: 0.3659123182296753\n",
      "\n",
      " Accuracy = 0.5617844781365258\n",
      "\n",
      "In step 19400, The loss is: 0.41635727882385254\n",
      "\n",
      " Accuracy = 0.5572750266558065\n",
      "\n",
      "In step 19500, The loss is: 0.6171268224716187\n",
      "\n",
      " Accuracy = 0.5416001065280109\n",
      "\n",
      "In step 19600, The loss is: 0.288502037525177\n",
      "\n",
      " Accuracy = 0.5521998135997845\n",
      "\n",
      "In step 19700, The loss is: 0.773793637752533\n",
      "\n",
      " Accuracy = 0.5445121139192733\n",
      "\n",
      "In step 19800, The loss is: 0.524029016494751\n",
      "\n",
      " Accuracy = 0.5574580670926518\n",
      "\n",
      "In step 19900, The loss is: 0.37645164132118225\n",
      "\n",
      " Accuracy = 0.5613684771350398\n",
      "\n",
      "In step 20000, The loss is: 0.30891770124435425\n",
      "\n",
      " Accuracy = 0.5571419062515417\n",
      "\n",
      "In step 20100, The loss is: 0.5022420883178711\n",
      "\n",
      " Accuracy = 0.549304446187644\n",
      "\n",
      "In step 20200, The loss is: 0.44952064752578735\n",
      "\n",
      " Accuracy = 0.5608692758570845\n",
      "\n",
      "In step 20300, The loss is: 0.42081862688064575\n",
      "\n",
      " Accuracy = 0.5611521565495208\n",
      "\n",
      "In step 20400, The loss is: 0.4972708225250244\n",
      "\n",
      " Accuracy = 0.5592385517141689\n",
      "\n",
      "In step 20500, The loss is: 0.5462361574172974\n",
      "\n",
      " Accuracy = 0.5444122736636823\n",
      "\n",
      "In step 20600, The loss is: 0.4131276607513428\n",
      "\n",
      " Accuracy = 0.5449447550903113\n",
      "\n",
      "In step 20700, The loss is: 0.3557792901992798\n",
      "\n",
      " Accuracy = 0.5520666932907349\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In step 20800, The loss is: 0.3241724967956543\n",
      "\n",
      " Accuracy = 0.538754659243666\n",
      "\n",
      "In step 20900, The loss is: 0.2669886648654938\n",
      "\n",
      " Accuracy = 0.5413671458681552\n",
      "\n",
      "In step 21000, The loss is: 0.5064041614532471\n",
      "\n",
      " Accuracy = 0.5568756656334423\n",
      "\n",
      "In step 21100, The loss is: 0.22831740975379944\n",
      "\n",
      " Accuracy = 0.5488052449096887\n",
      "\n",
      "In step 21200, The loss is: 0.5905050039291382\n",
      "\n",
      " Accuracy = 0.5599540734824281\n",
      "\n",
      "In step 21300, The loss is: 0.3659898638725281\n",
      "\n",
      " Accuracy = 0.5511015707882829\n",
      "\n",
      "In step 21400, The loss is: 0.35174766182899475\n",
      "\n",
      " Accuracy = 0.5551284611605989\n",
      "\n",
      "In step 21500, The loss is: 0.33016377687454224\n",
      "\n",
      " Accuracy = 0.5583732694673081\n",
      "\n",
      "In step 21600, The loss is: 0.4601675868034363\n",
      "\n",
      " Accuracy = 0.560070553812356\n",
      "\n",
      "In step 21700, The loss is: 0.1850145161151886\n",
      "\n",
      " Accuracy = 0.5353268104048964\n",
      "\n",
      "In step 21800, The loss is: 0.2835187613964081\n",
      "\n",
      " Accuracy = 0.5622670394353593\n",
      "\n",
      "In step 21900, The loss is: 0.3596092462539673\n",
      "\n",
      " Accuracy = 0.5607694356014934\n",
      "\n",
      "In step 22000, The loss is: 0.28841543197631836\n",
      "\n",
      " Accuracy = 0.5479566027371647\n",
      "\n",
      "In step 22100, The loss is: 0.42659130692481995\n",
      "\n",
      " Accuracy = 0.5511015707882829\n",
      "\n",
      "In step 22200, The loss is: 0.34249043464660645\n",
      "\n",
      " Accuracy = 0.5403853833865815\n",
      "\n",
      "In step 22300, The loss is: 0.42504698038101196\n",
      "\n",
      " Accuracy = 0.5545959797339698\n",
      "\n",
      "In step 22400, The loss is: 0.22705097496509552\n",
      "\n",
      " Accuracy = 0.5530484557723085\n",
      "\n",
      "In step 22500, The loss is: 0.46095746755599976\n",
      "\n",
      " Accuracy = 0.5456436368794487\n",
      "\n",
      "In step 22600, The loss is: 0.4455603361129761\n",
      "\n",
      " Accuracy = 0.5509185303514377\n",
      "\n",
      "In step 22700, The loss is: 0.39470773935317993\n",
      "\n",
      " Accuracy = 0.5384052183490973\n",
      "\n",
      "In step 22800, The loss is: 0.36701104044914246\n",
      "\n",
      " Accuracy = 0.5583399893186344\n",
      "\n",
      "In step 22900, The loss is: 0.40142592787742615\n",
      "\n",
      " Accuracy = 0.5538138977635783\n",
      "\n",
      "In step 23000, The loss is: 0.3248860239982605\n",
      "\n",
      " Accuracy = 0.5521498934719891\n",
      "\n",
      "In step 23100, The loss is: 0.15307185053825378\n",
      "\n",
      " Accuracy = 0.551733892470503\n",
      "\n",
      "In step 23200, The loss is: 0.43474793434143066\n",
      "\n",
      " Accuracy = 0.5586062300319489\n",
      "\n",
      "In step 23300, The loss is: 0.47166362404823303\n",
      "\n",
      " Accuracy = 0.5487220447284346\n",
      "\n",
      "In step 23400, The loss is: 0.29354241490364075\n",
      "\n",
      " Accuracy = 0.5428980298506947\n",
      "\n",
      "In step 23500, The loss is: 0.5101739168167114\n",
      "\n",
      " Accuracy = 0.5445453940679471\n",
      "\n",
      "In step 23600, The loss is: 0.31843772530555725\n",
      "\n",
      " Accuracy = 0.551617412140575\n",
      "\n",
      "In step 23700, The loss is: 0.5960102081298828\n",
      "\n",
      " Accuracy = 0.5485223642172524\n",
      "\n",
      "In step 23800, The loss is: 0.5857990980148315\n",
      "\n",
      " Accuracy = 0.5413005857612379\n",
      "\n",
      "In step 23900, The loss is: 0.527798593044281\n",
      "\n",
      " Accuracy = 0.546758519670072\n",
      "\n",
      "In step 24000, The loss is: 0.3223911225795746\n",
      "\n",
      " Accuracy = 0.5493876464641132\n",
      "\n",
      "In step 24100, The loss is: 0.19429272413253784\n",
      "\n",
      " Accuracy = 0.5377562566877554\n",
      "\n",
      "In step 24200, The loss is: 0.4649834632873535\n",
      "\n",
      " Accuracy = 0.5449613950694331\n",
      "\n",
      "In step 24300, The loss is: 0.4042830467224121\n",
      "\n",
      " Accuracy = 0.5466420394353593\n",
      "\n",
      "In step 24400, The loss is: 0.22966228425502777\n",
      "\n",
      " Accuracy = 0.5460929180296085\n",
      "\n",
      "In step 24500, The loss is: 0.2226998209953308\n",
      "\n",
      " Accuracy = 0.5420493876781708\n",
      "\n",
      "In step 24600, The loss is: 0.1825663447380066\n",
      "\n",
      " Accuracy = 0.533047124600639\n",
      "\n",
      "In step 24700, The loss is: 0.4833980202674866\n",
      "\n",
      " Accuracy = 0.5375732161556951\n",
      "\n",
      "In step 24800, The loss is: 0.21067526936531067\n",
      "\n",
      " Accuracy = 0.5400026624385541\n",
      "\n",
      "In step 24900, The loss is: 0.2613571286201477\n",
      "\n",
      " Accuracy = 0.5366580138762538\n",
      "\n",
      "In step 25000, The loss is: 0.5377269983291626\n",
      "\n",
      " Accuracy = 0.5474241214057508\n",
      "\n",
      "In step 25100, The loss is: 0.28671082854270935\n",
      "\n",
      " Accuracy = 0.5448449148347203\n",
      "\n",
      "In step 25200, The loss is: 0.20406055450439453\n",
      "\n",
      " Accuracy = 0.5477402822468609\n",
      "\n",
      "In step 25300, The loss is: 0.4482842683792114\n",
      "\n",
      " Accuracy = 0.5438964324066052\n",
      "\n",
      "In step 25400, The loss is: 0.2605479657649994\n",
      "\n",
      " Accuracy = 0.5547790202660302\n",
      "\n",
      "In step 25500, The loss is: 0.4395748972892761\n",
      "\n",
      " Accuracy = 0.544345713556765\n",
      "\n",
      "In step 25600, The loss is: 0.3487856686115265\n",
      "\n",
      " Accuracy = 0.5408346645367412\n",
      "\n",
      "In step 25700, The loss is: 0.3959507942199707\n",
      "\n",
      " Accuracy = 0.5411009052500557\n",
      "\n",
      "In step 25800, The loss is: 0.3906116187572479\n",
      "\n",
      " Accuracy = 0.5426484292117171\n",
      "\n",
      "In step 25900, The loss is: 0.22305604815483093\n",
      "\n",
      " Accuracy = 0.5400026624385541\n",
      "\n",
      "In step 26000, The loss is: 0.23508799076080322\n",
      "\n",
      " Accuracy = 0.533779286443235\n",
      "\n",
      "In step 26100, The loss is: 0.3508920967578888\n",
      "\n",
      " Accuracy = 0.5340788072100082\n",
      "\n",
      "In step 26200, The loss is: 0.29965847730636597\n",
      "\n",
      " Accuracy = 0.5489550052930753\n",
      "\n",
      "In step 26300, The loss is: 0.17886385321617126\n",
      "\n",
      " Accuracy = 0.5494542065710305\n",
      "\n",
      "In step 26400, The loss is: 0.15648435056209564\n",
      "\n",
      " Accuracy = 0.5390375399361023\n",
      "\n",
      "In step 26500, The loss is: 0.3893798589706421\n",
      "\n",
      " Accuracy = 0.5510682907348243\n",
      "\n",
      "In step 26600, The loss is: 0.22518861293792725\n",
      "\n",
      " Accuracy = 0.5469582001812542\n",
      "\n",
      "In step 26700, The loss is: 0.28606048226356506\n",
      "\n",
      " Accuracy = 0.5419994675503752\n",
      "\n",
      "In step 26800, The loss is: 0.4336102604866028\n",
      "\n",
      " Accuracy = 0.5522330937484583\n",
      "\n",
      "In step 26900, The loss is: 0.17033478617668152\n",
      "\n",
      " Accuracy = 0.5384884185303515\n",
      "\n",
      "In step 27000, The loss is: 0.39354953169822693\n",
      "\n",
      " Accuracy = 0.5381389776357828\n",
      "\n",
      "In step 27100, The loss is: 0.25611814856529236\n",
      "\n",
      " Accuracy = 0.5445287539936102\n",
      "\n",
      "In step 27200, The loss is: 0.3042079508304596\n",
      "\n",
      " Accuracy = 0.5463757987220448\n",
      "\n",
      "In step 27300, The loss is: 0.2064056694507599\n",
      "\n",
      " Accuracy = 0.5423156282962701\n",
      "\n",
      "In step 27400, The loss is: 0.35906362533569336\n",
      "\n",
      " Accuracy = 0.5403188231844491\n",
      "\n",
      "In step 27500, The loss is: 0.30374619364738464\n",
      "\n",
      " Accuracy = 0.5364416932907349\n",
      "\n",
      "In step 27600, The loss is: 0.3514097332954407\n",
      "\n",
      " Accuracy = 0.5380724174336503\n",
      "\n",
      "In step 27700, The loss is: 0.2681105136871338\n",
      "\n",
      " Accuracy = 0.5482228434504792\n",
      "\n",
      "In step 27800, The loss is: 0.3133167028427124\n",
      "\n",
      " Accuracy = 0.5385383386581469\n",
      "\n",
      "In step 27900, The loss is: 0.28169476985931396\n",
      "\n",
      " Accuracy = 0.5367578541318448\n",
      "\n",
      "In step 28000, The loss is: 0.45308682322502136\n",
      "\n",
      " Accuracy = 0.5349940095846646\n",
      "\n",
      "In step 28100, The loss is: 0.24066081643104553\n",
      "\n",
      " Accuracy = 0.5461594781365258\n",
      "\n",
      "end.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = AttentionRNN(reversed_dictionary, doc_max_len, num_class)\n",
    "\n",
    "    train_batches = batch_iter(train_x, train_y, 64, 10)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // 64 + 1\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    for x_batch, y_batch in train_batches:\n",
    "        train_feed_dict = {\n",
    "            model.x: x_batch,\n",
    "            model.y: y_batch,\n",
    "            model.keep_prob: 0.8\n",
    "        }\n",
    "        _, step, loss = sess.run([model.optimizer, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"In step {0}, The loss is: {1}\".format(step, loss))\n",
    "        if step % 100 == 0:\n",
    "            valid_batches = batch_iter(valid_x, valid_y, 64, 1)\n",
    "            sum_accuracy, count = 0, 0\n",
    "            for valid_x_batch, valid_y_batch in valid_batches:\n",
    "                valid_feed_dict = {\n",
    "                    model.x: valid_x_batch,\n",
    "                    model.y: valid_y_batch,\n",
    "                    model.keep_prob: 1.0\n",
    "                }\n",
    "                accuracy = sess.run(model.accuracy, feed_dict=valid_feed_dict)\n",
    "                sum_accuracy += accuracy\n",
    "                count += 1\n",
    "            temp_accuracy = sum_accuracy / count\n",
    "\n",
    "            print(\"\\n Accuracy = {1}\\n\".format(step // num_batches_per_epoch, sum_accuracy / count))\n",
    "            # Save the model that can produce the best accuracy\n",
    "            if temp_accuracy > best_accuracy:\n",
    "                best_accuracy = temp_accuracy\n",
    "                model_saver.save(sess, \"{0}/{1}.ckpt\".format(\"saved_model\", \"naive\"), global_step=step)\n",
    "    \n",
    "    print(\"end.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
