{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['nohup.out', '.zshrc', '.bash_logout', 'glove.6B.300d.txt', 'examples', '.ipython', 'README', '.ssh', '.config', '.conda', 'src', '.sudo_as_admin_successful', '.dl_binaries', '.bash_history', '.cmake', '.jupyter', 'ssl', '.local', '.cache', '.glove.6B.300d.txt.swp', 'Nvidia_Cloud_EULA.pdf', '.dlamirc', 'test_data.json', '.rnd', 'glove100.txt', '.viminfo', '.condarc', '.keras', 'test1.json', 'tf_stanford', '.bashrc', '.profile', 'anaconda2', '.ipynb_checkpoints', 'tutorials', 'nltk_data', 'anaconda3', 'glove.6B.50d.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.contrib import rnn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import os\n",
    "print(os.listdir(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('~/test_data.json', lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 41 category\n",
      "category\n",
      "ARTS               1509\n",
      "ARTS & CULTURE     1339\n",
      "BLACK VOICES       4528\n",
      "BUSINESS           5937\n",
      "COLLEGE            1144\n",
      "COMEDY             5175\n",
      "CRIME              3405\n",
      "CULTURE & ARTS     1030\n",
      "DIVORCE            3426\n",
      "EDUCATION          1004\n",
      "ENTERTAINMENT     16058\n",
      "ENVIRONMENT        1323\n",
      "FIFTY              1401\n",
      "FOOD & DRINK       6226\n",
      "GOOD NEWS          1398\n",
      "GREEN              2622\n",
      "HEALTHY LIVING     6694\n",
      "HOME & LIVING      4195\n",
      "IMPACT             3459\n",
      "LATINO VOICES      1129\n",
      "MEDIA              2815\n",
      "MONEY              1707\n",
      "PARENTING          8677\n",
      "PARENTS            3955\n",
      "POLITICS          32739\n",
      "QUEER VOICES       6314\n",
      "RELIGION           2556\n",
      "SCIENCE            2178\n",
      "SPORTS             4884\n",
      "STYLE              2254\n",
      "STYLE & BEAUTY     9649\n",
      "TASTE              2096\n",
      "TECH               2082\n",
      "THE WORLDPOST      3664\n",
      "TRAVEL             9887\n",
      "WEDDINGS           3651\n",
      "WEIRD NEWS         2670\n",
      "WELLNESS          17827\n",
      "WOMEN              3490\n",
      "WORLD NEWS         2177\n",
      "WORLDPOST          2579\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cates = data.groupby('category')\n",
    "print(\"There are:\", cates.ngroups, \"category\")\n",
    "print(cates.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category = data.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>[74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>[42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>[5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>[2710, 13374, 3596, 64143, 2295, 13055, 5, 569...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>[41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                               words  word_length  \n",
       "0  [74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...           27  \n",
       "1  [42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...           20  \n",
       "2  [5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...           25  \n",
       "3  [2710, 13374, 3596, 64143, 2295, 13055, 5, 569...           26  \n",
       "4  [41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...           26  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Change the text to tokens.\n",
    "data['text'] = data.headline + \" \" + data.short_description\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.text)\n",
    "X = tokenizer.texts_to_sequences(data.text)\n",
    "data['words'] = X\n",
    "\n",
    "data['word_length'] = data.words.apply(lambda i: len(i))\n",
    "data = data[data.word_length >= 5]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    199914.000000\n",
       "mean         29.725032\n",
       "std          14.024717\n",
       "min           5.000000\n",
       "25%          20.000000\n",
       "50%          29.000000\n",
       "75%          36.000000\n",
       "max         248.000000\n",
       "Name: word_length, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.word_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "X = list(sequence.pad_sequences(data.words, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "categories = data.groupby('category').size().index.tolist()\n",
    "category_int = {}\n",
    "int_category = {}\n",
    "# build category to int dict and int to category dict.\n",
    "for i, k in enumerate(categories):\n",
    "    int_category.update({i:k})\n",
    "    category_int.update({k:i})\n",
    "                                    \n",
    "data['c2id'] = data['category'].apply(lambda x: category_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>word_length</th>\n",
       "      <th>c2id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>[74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>[42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>[5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>[2710, 13374, 3596, 64143, 2295, 13055, 5, 569...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>[41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                               words  word_length  c2id  \n",
       "0  [74, 101, 257, 1331, 3001, 6, 698, 134, 96, 26...           27     6  \n",
       "1  [42, 1604, 2960, 27762, 5, 25929, 5237, 8, 1, ...           20    10  \n",
       "2  [5877, 5334, 8083, 8, 1, 76, 54, 21, 414, 8469...           25    10  \n",
       "3  [2710, 13374, 3596, 64143, 2295, 13055, 5, 569...           26    10  \n",
       "4  [41003, 36082, 1513, 97, 48, 7915, 3134, 2, 96...           26    10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_setence(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93757\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = data[\"text\"]\n",
    "\n",
    "words = list()\n",
    "for test in texts:\n",
    "    for word in word_tokenize(clean_setence(test)):\n",
    "        words.append(word)\n",
    "\n",
    "word_counter = collections.Counter(words).most_common()\n",
    "word_dictionary = dict()\n",
    "word_dictionary[\"<padding>\"] = 0\n",
    "word_dictionary[\"<unk>\"] = 1\n",
    "for word, _ in word_counter:\n",
    "    word_dictionary[word] = len(word_dictionary)\n",
    "\n",
    "with open(\"word_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word_dictionary, f)\n",
    "\n",
    "reversed_dictionary = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "print(len(reversed_dictionary))\n",
    "doc_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Split data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(lambda d: word_tokenize(clean_setence(d)), data[\"text\"]))\n",
    "x = list(map(lambda d: list(map(lambda w: word_dictionary.get(w, word_dictionary[\"<unk>\"]), d)), x))\n",
    "x = list(map(lambda d: d[:doc_max_len], x))\n",
    "x = list(map(lambda d: d + (doc_max_len - len(d)) * [word_dictionary[\"<padding>\"]], x))\n",
    "\n",
    "y = list(data[\"c2id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test set are prepared.\n"
     ]
    }
   ],
   "source": [
    "print(\"train and test set are prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalRNN(object):\n",
    "    def __init__(self, reversed_dictionary, doc_max_len, num_class):\n",
    "        self.x = tf.placeholder(tf.int32, [None, doc_max_len], name=\"x\")\n",
    "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
    "        self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        print(\"Run BidirectionalRNN...\")\n",
    "        self.vocabulary_size = len(reversed_dictionary)\n",
    "        self.embedding_size = 300\n",
    "        self.num_hidden = 100\n",
    "        self.num_layers = 2\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        print(\"Loading Glove vectors...\")\n",
    "        glove_file = \"../glove.6B.%dd.txt\" % self.embedding_size\n",
    "        word2vec_file = get_tmpfile(\"word2vec_format.vec\")\n",
    "        glove2word2vec(glove_file, word2vec_file)\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
    "\n",
    "        word_vec_list = list()\n",
    "        for _, word in sorted(reversed_dictionary.items()):\n",
    "            try:\n",
    "                word_vec = word_vectors.word_vec(word)\n",
    "            except KeyError:\n",
    "                word_vec = np.zeros([self.embedding_size], dtype=np.float32)\n",
    "\n",
    "            word_vec_list.append(word_vec)\n",
    "\n",
    "\n",
    "        print(\"start embedding...\")\n",
    "        init_embeddings = tf.constant(np.array(word_vec_list), dtype=tf.float32)\n",
    "        self.embeddings1 = tf.get_variable(\"embeddings1\", initializer=init_embeddings, trainable=True)\n",
    "        self.x_emb = tf.nn.embedding_lookup(self.embeddings1, self.x)\n",
    "\n",
    "        forward_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "        bw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "        forward_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in forward_cells]\n",
    "        bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "        self.rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n",
    "            forward_cells, bw_cells, self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
    "        self.last_output = self.rnn_outputs[:, -1, :]\n",
    "        self.logits = tf.contrib.slim.fully_connected(self.last_output, num_class, activation_fn=None)\n",
    "        self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "        # make prediction\n",
    "        correct_predictions = tf.equal(self.predictions, self.y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(inputs, outputs, batch_size, epochs_num):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(epochs_num):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run BidirectionalRNN...\n",
      "Loading Glove vectors...\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = BidirectionalRNN(reversed_dictionary, doc_max_len, num_class)\n",
    "\n",
    "    train_batches = batch_iter(train_x, train_y, 64, 10)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // 64 + 1\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    for x_batch, y_batch in train_batches:\n",
    "        train_feed_dict = {\n",
    "            model.x: x_batch,\n",
    "            model.y: y_batch,\n",
    "            model.keep_prob: 0.8\n",
    "        }\n",
    "        _, step, loss = sess.run([model.optimizer, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"In step {0}, The loss is: {1}\".format(step, loss))\n",
    "        if step % 100 == 0:\n",
    "            valid_batches = batch_iter(valid_x, valid_y, 64, 1)\n",
    "            sum_accuracy, count = 0, 0\n",
    "            for valid_x_batch, valid_y_batch in valid_batches:\n",
    "                valid_feed_dict = {\n",
    "                    model.x: valid_x_batch,\n",
    "                    model.y: valid_y_batch,\n",
    "                    model.keep_prob: 1.0\n",
    "                }\n",
    "                accuracy = sess.run(model.accuracy, feed_dict=valid_feed_dict)\n",
    "                sum_accuracy += accuracy\n",
    "                count += 1\n",
    "            temp_accuracy = sum_accuracy / count\n",
    "\n",
    "            print(\"\\n Accuracy = {1}\\n\".format(step // num_batches_per_epoch, sum_accuracy / count))\n",
    "            # Save the model that can produce the best accuracy\n",
    "            if temp_accuracy > best_accuracy:\n",
    "                best_accuracy = temp_accuracy\n",
    "                model_saver.save(sess, \"{0}/{1}.ckpt\".format(\"saved_model\", \"naive\"), global_step=step)\n",
    "    \n",
    "    print(\"end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNN(object):\n",
    "    def __init__(self, reversed_dict, document_max_len, num_class):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = 300\n",
    "        self.num_hidden = 100\n",
    "        self.num_layers = 2\n",
    "        self.learning_rate = 1e-3\n",
    "\n",
    "        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=\"x\")\n",
    "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
    "        self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"keep_prob\")\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size])\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings, trainable=True)\n",
    "            self.x_emb = tf.nn.embedding_lookup(self.embeddings, self.x)\n",
    "\n",
    "        with tf.name_scope(\"birnn\"):\n",
    "            fw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "            self.rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
    "\n",
    "        with tf.name_scope(\"attention\"):\n",
    "            self.attention_score = tf.nn.softmax(tf.contrib.slim.fully_connected(self.rnn_outputs, 1))\n",
    "            self.attention_out = tf.squeeze(\n",
    "                tf.matmul(tf.transpose(self.rnn_outputs, perm=[0, 2, 1]), self.attention_score),\n",
    "                axis=-1)\n",
    "\n",
    "        with tf.name_scope(\"output\"):\n",
    "            self.logits = tf.contrib.slim.fully_connected(self.attention_out, num_class, activation_fn=None)\n",
    "            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, self.y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = AttentionRNN(reversed_dictionary, doc_max_len, num_class)\n",
    "\n",
    "    train_batches = batch_iter(train_x, train_y, 64, 10)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // 64 + 1\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    for x_batch, y_batch in train_batches:\n",
    "        train_feed_dict = {\n",
    "            model.x: x_batch,\n",
    "            model.y: y_batch,\n",
    "            model.keep_prob: 0.8\n",
    "        }\n",
    "        _, step, loss = sess.run([model.optimizer, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"In step {0}, The loss is: {1}\".format(step, loss))\n",
    "        if step % 100 == 0:\n",
    "            valid_batches = batch_iter(valid_x, valid_y, 64, 1)\n",
    "            sum_accuracy, count = 0, 0\n",
    "            for valid_x_batch, valid_y_batch in valid_batches:\n",
    "                valid_feed_dict = {\n",
    "                    model.x: valid_x_batch,\n",
    "                    model.y: valid_y_batch,\n",
    "                    model.keep_prob: 1.0\n",
    "                }\n",
    "                accuracy = sess.run(model.accuracy, feed_dict=valid_feed_dict)\n",
    "                sum_accuracy += accuracy\n",
    "                count += 1\n",
    "            temp_accuracy = sum_accuracy / count\n",
    "\n",
    "            print(\"\\n Accuracy = {1}\\n\".format(step // num_batches_per_epoch, sum_accuracy / count))\n",
    "            # Save the model that can produce the best accuracy\n",
    "            if temp_accuracy > best_accuracy:\n",
    "                best_accuracy = temp_accuracy\n",
    "                model_saver.save(sess, \"{0}/{1}.ckpt\".format(\"saved_model\", \"naive\"), global_step=step)\n",
    "    \n",
    "    print(\"end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['c2id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  CRIME\n",
       "1          ENTERTAINMENT\n",
       "2          ENTERTAINMENT\n",
       "3          ENTERTAINMENT\n",
       "4          ENTERTAINMENT\n",
       "5          ENTERTAINMENT\n",
       "6          ENTERTAINMENT\n",
       "7          ENTERTAINMENT\n",
       "8          ENTERTAINMENT\n",
       "9          ENTERTAINMENT\n",
       "10         ENTERTAINMENT\n",
       "11            WORLD NEWS\n",
       "12                IMPACT\n",
       "13              POLITICS\n",
       "14              POLITICS\n",
       "15              POLITICS\n",
       "16              POLITICS\n",
       "17              POLITICS\n",
       "18              POLITICS\n",
       "19              POLITICS\n",
       "20            WEIRD NEWS\n",
       "21         ENTERTAINMENT\n",
       "22            WEIRD NEWS\n",
       "23            WORLD NEWS\n",
       "24            WORLD NEWS\n",
       "25            WORLD NEWS\n",
       "26            WORLD NEWS\n",
       "27          BLACK VOICES\n",
       "28          BLACK VOICES\n",
       "29          BLACK VOICES\n",
       "               ...      \n",
       "200823      QUEER VOICES\n",
       "200824      QUEER VOICES\n",
       "200825            IMPACT\n",
       "200826            IMPACT\n",
       "200827            IMPACT\n",
       "200828       ENVIRONMENT\n",
       "200829       ENVIRONMENT\n",
       "200830       ENVIRONMENT\n",
       "200831       ENVIRONMENT\n",
       "200832       ENVIRONMENT\n",
       "200833          BUSINESS\n",
       "200834          BUSINESS\n",
       "200835          BUSINESS\n",
       "200836          BUSINESS\n",
       "200837          BUSINESS\n",
       "200838     ENTERTAINMENT\n",
       "200839     ENTERTAINMENT\n",
       "200840    CULTURE & ARTS\n",
       "200841    CULTURE & ARTS\n",
       "200842    CULTURE & ARTS\n",
       "200843              TECH\n",
       "200844              TECH\n",
       "200845              TECH\n",
       "200846              TECH\n",
       "200847              TECH\n",
       "200848              TECH\n",
       "200849            SPORTS\n",
       "200850            SPORTS\n",
       "200851            SPORTS\n",
       "200852            SPORTS\n",
       "Name: category, Length: 199914, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
