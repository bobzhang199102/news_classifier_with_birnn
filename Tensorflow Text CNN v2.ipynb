{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['nohup.out', '.zshrc', '.bash_logout', 'glove.6B.300d.txt', 'examples', '.ipython', 'README', '.ssh', '.config', '.conda', 'src', '.sudo_as_admin_successful', '.dl_binaries', '.bash_history', '.cmake', '.jupyter', 'ssl', '.local', '.cache', '.glove.6B.300d.txt.swp', 'Nvidia_Cloud_EULA.pdf', '.dlamirc', 'test_data.json', '.rnd', 'glove100.txt', '.viminfo', '.condarc', '.keras', 'test1.json', 'tf_stanford', '.bashrc', '.profile', 'anaconda2', '.ipynb_checkpoints', 'tutorials', 'nltk_data', 'anaconda3', 'glove.6B.50d.txt']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.contrib import rnn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import os\n",
    "print(os.listdir(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \n",
       "0  She left her husband. He killed their children...  \n",
       "1                           Of course it has a song.  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  \n",
       "4  The \"Dietland\" actress said using the bags is ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('~/test1.json', lines=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 19 category\n",
      "category\n",
      "BLACK VOICES      55\n",
      "BUSINESS           8\n",
      "COMEDY            70\n",
      "CRIME             23\n",
      "EDUCATION          1\n",
      "ENTERTAINMENT    237\n",
      "IMPACT            11\n",
      "LATINO VOICES      4\n",
      "MEDIA             23\n",
      "POLITICS         368\n",
      "QUEER VOICES      52\n",
      "RELIGION          11\n",
      "SCIENCE            4\n",
      "SPORTS            11\n",
      "TECH              12\n",
      "TRAVEL             8\n",
      "WEIRD NEWS        27\n",
      "WOMEN             19\n",
      "WORLD NEWS        56\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cates = data.groupby('category')\n",
    "print(\"There are:\", cates.ngroups, \"category\")\n",
    "print(cates.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.category = data.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>[201, 134, 219, 583, 909, 5, 127, 168, 109, 34...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>[37, 492, 1672, 2774, 6, 2775, 2776, 7, 1, 305...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>[1176, 2779, 2780, 7, 1, 55, 89, 12, 910, 2781...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>[307, 714, 2784, 2785, 584, 2786, 6, 220, 5, 1...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>[2789, 2790, 914, 46, 10, 2791, 1179, 2, 425, ...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                               words  word_length  \n",
       "0  [201, 134, 219, 583, 909, 5, 127, 168, 109, 34...           27  \n",
       "1  [37, 492, 1672, 2774, 6, 2775, 2776, 7, 1, 305...           20  \n",
       "2  [1176, 2779, 2780, 7, 1, 55, 89, 12, 910, 2781...           25  \n",
       "3  [307, 714, 2784, 2785, 584, 2786, 6, 220, 5, 1...           26  \n",
       "4  [2789, 2790, 914, 46, 10, 2791, 1179, 2, 425, ...           26  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Change the text to tokens.\n",
    "data['text'] = data.headline + \" \" + data.short_description\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.text)\n",
    "X = tokenizer.texts_to_sequences(data.text)\n",
    "data['words'] = X\n",
    "\n",
    "data['word_length'] = data.words.apply(lambda i: len(i))\n",
    "data = data[data.word_length >= 5]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean       25.413000\n",
       "std         6.417446\n",
       "min         8.000000\n",
       "25%        21.000000\n",
       "50%        25.000000\n",
       "75%        30.000000\n",
       "max        50.000000\n",
       "Name: word_length, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.word_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 50\n",
    "X = list(sequence.pad_sequences(data.words, maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = data.groupby('category').size().index.tolist()\n",
    "category_int = {}\n",
    "int_category = {}\n",
    "# build category to int dict and int to category dict.\n",
    "for i, k in enumerate(categories):\n",
    "    int_category.update({i:k})\n",
    "    category_int.update({k:i})\n",
    "\n",
    "data['c2id'] = data['category'].apply(lambda x: category_int[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>word_length</th>\n",
       "      <th>c2id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>CRIME</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>[201, 134, 219, 583, 909, 5, 127, 168, 109, 34...</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>[37, 492, 1672, 2774, 6, 2775, 2776, 7, 1, 305...</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 5...</td>\n",
       "      <td>[1176, 2779, 2780, 7, 1, 55, 89, 12, 910, 2781...</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>[307, 714, 2784, 2785, 584, 2786, 6, 220, 5, 1...</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2018-05-26</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>[2789, 2790, 914, 46, 10, 2791, 1179, 2, 425, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           authors       category       date  \\\n",
       "0  Melissa Jeltsen          CRIME 2018-05-26   \n",
       "1    Andy McDonald  ENTERTAINMENT 2018-05-26   \n",
       "2       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "3       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "4       Ron Dicker  ENTERTAINMENT 2018-05-26   \n",
       "\n",
       "                                            headline  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description  \\\n",
       "0  She left her husband. He killed their children...   \n",
       "1                           Of course it has a song.   \n",
       "2  The actor and his longtime girlfriend Anna Ebe...   \n",
       "3  The actor gives Dems an ass-kicking for not fi...   \n",
       "4  The \"Dietland\" actress said using the bags is ...   \n",
       "\n",
       "                                                text  \\\n",
       "0  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  Hugh Grant Marries For The First Time At Age 5...   \n",
       "3  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "                                               words  word_length  c2id  \n",
       "0  [201, 134, 219, 583, 909, 5, 127, 168, 109, 34...           27     3  \n",
       "1  [37, 492, 1672, 2774, 6, 2775, 2776, 7, 1, 305...           20     5  \n",
       "2  [1176, 2779, 2780, 7, 1, 55, 89, 12, 910, 2781...           25     5  \n",
       "3  [307, 714, 2784, 2785, 584, 2786, 6, 220, 5, 1...           26     5  \n",
       "4  [2789, 2790, 914, 46, 10, 2791, 1179, 2, 425, ...           26     5  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_setence(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "texts = data[\"text\"]\n",
    "\n",
    "words = list()\n",
    "for test in texts:\n",
    "    for word in word_tokenize(clean_setence(test)):\n",
    "        words.append(word)\n",
    "\n",
    "word_counter = collections.Counter(words).most_common()\n",
    "word_dictionary = dict()\n",
    "word_dictionary[\"<padding>\"] = 0\n",
    "word_dictionary[\"<unk>\"] = 1\n",
    "for word, _ in word_counter:\n",
    "    word_dictionary[word] = len(word_dictionary)\n",
    "\n",
    "with open(\"word_dict.pickle\", \"wb\") as f:\n",
    "    pickle.dump(word_dictionary, f)\n",
    "\n",
    "reversed_dictionary = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "print(len(reversed_dictionary))\n",
    "doc_max_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Split data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(lambda d: word_tokenize(clean_str(d)), data[\"text\"]))\n",
    "x = list(map(lambda d: list(map(lambda w: word_dictionary.get(w, word_dictionary[\"<unk>\"]), d)), x))\n",
    "x = list(map(lambda d: d[:doc_max_len], x))\n",
    "x = list(map(lambda d: d + (doc_max_len - len(d)) * [word_dictionary[\"<padding>\"]], x))\n",
    "\n",
    "y = list(data[\"c2id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test set are prepared.\n"
     ]
    }
   ],
   "source": [
    "print(\"train and test set are prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalRNN(object):\n",
    "    def __init__(self, reversed_dictionary, doc_max_len, num_class):\n",
    "        self.x = tf.placeholder(tf.int32, [None, doc_max_len], name=\"x\")\n",
    "        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n",
    "        self.y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name='keep_prob')\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        print(\"Run BidirectionalRNN...\")\n",
    "        self.vocabulary_size = len(reversed_dictionary)\n",
    "        self.embedding_size = 300\n",
    "        self.num_hidden = 100\n",
    "        self.num_layers = 2\n",
    "        self.learning_rate = 1e-3\n",
    "        \n",
    "        print(\"Loading Glove vectors...\")\n",
    "        glove_file = \"../glove.6B.%dd.txt\" % self.embedding_size\n",
    "        word2vec_file = get_tmpfile(\"word2vec_format.vec\")\n",
    "        glove2word2vec(glove_file, word2vec_file)\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
    "\n",
    "        word_vec_list = list()\n",
    "        for _, word in sorted(reversed_dictionary.items()):\n",
    "            try:\n",
    "                word_vec = word_vectors.word_vec(word)\n",
    "            except KeyError:\n",
    "                word_vec = np.zeros([self.embedding_size], dtype=np.float32)\n",
    "\n",
    "            word_vec_list.append(word_vec)\n",
    "\n",
    "\n",
    "        print(\"start embedding...\")\n",
    "        init_embeddings = tf.constant(np.array(word_vec_list), dtype=tf.float32)\n",
    "        self.embeddings1 = tf.get_variable(\"embeddings1\", initializer=init_embeddings, trainable=True)\n",
    "        self.x_emb = tf.nn.embedding_lookup(self.embeddings1, self.x)\n",
    "\n",
    "        fw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "        bw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "        fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n",
    "        bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n",
    "\n",
    "        self.rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n",
    "            fw_cells, bw_cells, self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
    "        self.last_output = self.rnn_outputs[:, -1, :]\n",
    "        self.logits = tf.contrib.slim.fully_connected(self.last_output, num_class, activation_fn=None)\n",
    "        self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n",
    "        self.loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "        # make prediction\n",
    "        correct_predictions = tf.equal(self.predictions, self.y)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def batch_iter(inputs, outputs, batch_size, epochs_num):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(epochs_num):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run BidirectionalRNN...\n",
      "Loading Glove vectors...\n",
      "start embedding...\n",
      "In step 100, The loss is: 1.538771629333496\n",
      "\n",
      " Accuracy = 0.5677083432674408\n",
      "\n",
      "end.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = BidirectionalRNN(reversed_dictionary, doc_max_len, num_class)\n",
    "\n",
    "    train_batches = batch_iter(train_x, train_y, 64, 10)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // 64 + 1\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    for x_batch, y_batch in train_batches:\n",
    "        train_feed_dict = {\n",
    "            model.x: x_batch,\n",
    "            model.y: y_batch,\n",
    "            model.keep_prob: 0.8\n",
    "        }\n",
    "        _, step, loss = sess.run([model.optimizer, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print(\"In step {0}, The loss is: {1}\".format(step, loss))\n",
    "        if step % 100 == 0:\n",
    "            valid_batches = batch_iter(valid_x, valid_y, 64, 1)\n",
    "            sum_accuracy, count = 0, 0\n",
    "            for valid_x_batch, valid_y_batch in valid_batches:\n",
    "                valid_feed_dict = {\n",
    "                    model.x: valid_x_batch,\n",
    "                    model.y: valid_y_batch,\n",
    "                    model.keep_prob: 1.0\n",
    "                }\n",
    "                accuracy = sess.run(model.accuracy, feed_dict=valid_feed_dict)\n",
    "                sum_accuracy += accuracy\n",
    "                count += 1\n",
    "            temp_accuracy = sum_accuracy / count\n",
    "\n",
    "            print(\"\\n Accuracy = {1}\\n\".format(step // num_batches_per_epoch, sum_accuracy / count))\n",
    "            # Save the model that can produce the best accuracy\n",
    "            if temp_accuracy > best_accuracy:\n",
    "                best_accuracy = temp_accuracy\n",
    "                model_saver.save(sess, \"{0}/{1}.ckpt\".format(\"saved_model\", \"naive\"), global_step=step)\n",
    "    \n",
    "    print(\"end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
